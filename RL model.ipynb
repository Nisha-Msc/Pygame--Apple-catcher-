{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefee727-fe26-4375-9943-b876415370bc",
   "metadata": {},
   "source": [
    "# Reinforcement model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723ad51-27c7-4616-a85b-4865b3abc727",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5ad8ac-73de-43dc-853a-44def9a00745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\program files\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in d:\\program files\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\program files\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in d:\\program files\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\program files\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\program files\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in d:\\program files\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\program files\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\program files\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\program files\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38e864fd-a929-4da1-a9b6-de8d57bfa016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Score: 94, Lives: 0, Rewards: 89\n",
      "Actions taken: ['stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'stay', 'stay', 'stay', 'stay', 'right', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'left', 'left', 'left', 'stay', 'stay', 'stay', 'stay', 'stay', 'right', 'right', 'right', 'right', 'left', 'left', 'stay', 'stay', 'stay', 'stay', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left']\n",
      "Episode 2: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'left', 'left', 'right', 'right']\n",
      "Episode 3: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'left', 'right', 'stay', 'stay']\n",
      "Episode 4: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'left', 'left', 'left', 'left']\n",
      "Episode 5: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right']\n",
      "Episode 6: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right']\n",
      "Episode 7: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right']\n",
      "Episode 8: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'stay', 'stay', 'stay', 'stay']\n",
      "Episode 9: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right']\n",
      "Episode 10: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'left']\n",
      "Episode 11: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['left', 'right', 'right', 'right', 'right']\n",
      "Episode 12: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['left', 'stay', 'stay', 'stay', 'stay']\n",
      "Episode 13: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'right', 'right', 'right', 'right']\n",
      "Episode 14: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'left', 'left', 'left']\n",
      "Episode 15: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['left', 'left', 'left', 'right', 'right']\n",
      "Episode 16: Score: 14, Lives: 0, Rewards: 9\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right']\n",
      "Episode 17: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'right', 'right', 'right']\n",
      "Episode 18: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'stay', 'stay', 'stay', 'stay']\n",
      "Episode 19: Score: 44, Lives: 0, Rewards: 39\n",
      "Actions taken: ['stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'left', 'left', 'stay', 'left', 'left', 'left', 'left', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'stay', 'left', 'left', 'left', 'left', 'left', 'left']\n",
      "Episode 20: Score: 0, Lives: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'stay', 'stay', 'stay', 'stay']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.001    # Learning rate for the neural network\n",
    "gamma = 0.9      # Discount factor\n",
    "epsilon = 0.1    # Exploration rate\n",
    "episodes = 20\n",
    "max_steps = 100  # Max steps per episode\n",
    "batch_size = 64  # Batch size for training\n",
    "replay_buffer_size = 10000\n",
    "update_target_every = 10  # Update target network every N episodes\n",
    "\n",
    "# Action space (left, right, stay)\n",
    "actions = ['left', 'right', 'stay']\n",
    "action_map = {action: idx for idx, action in enumerate(actions)}  # Map actions to indices\n",
    "\n",
    "# Screen dimensions\n",
    "WIDTH = 800\n",
    "HEIGHT = 600\n",
    "basket_width = 100\n",
    "basket_speed = 10\n",
    "apple_width = 20\n",
    "\n",
    "# Neural Network Model for Q-value approximation\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Q-network and target network\n",
    "q_network = DQN(2, len(actions))  # Input: state (basket_x, apple_x), Output: Q-values for actions\n",
    "target_network = DQN(2, len(actions))  # Target network\n",
    "target_network.load_state_dict(q_network.state_dict())  # Copy weights\n",
    "target_network.eval()  # Set to evaluation mode\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=alpha)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "replay_buffer = []\n",
    "\n",
    "def get_state(basket_x, apple_x):\n",
    "    \"\"\"Get the current state, represented as the basket's x-position and apple's x-position\"\"\"\n",
    "    return np.array([basket_x, apple_x], dtype=np.float32)\n",
    "\n",
    "def choose_action(state):\n",
    "    \"\"\"Epsilon-greedy strategy to choose action\"\"\"\n",
    "    state = torch.tensor(state).unsqueeze(0)  # Add batch dimension\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # Explore\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(state)\n",
    "        return actions[q_values.argmax().item()]  # Exploit\n",
    "\n",
    "def update_q_network(batch):\n",
    "    \"\"\"Update the Q-network using a batch of experiences\"\"\"\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "    \n",
    "    states = torch.tensor(np.array(states))  # Convert list of numpy arrays to a single numpy array\n",
    "    next_states = torch.tensor(np.array(next_states))  # Same as above\n",
    "    \n",
    "    # Convert actions to indices based on the action space\n",
    "    action_indices = [action_map[action] for action in actions]  # Use action_map for correct indices\n",
    "    \n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool)\n",
    "    \n",
    "    # Get current Q-values from Q-network\n",
    "    q_values = q_network(states)\n",
    "    next_q_values = target_network(next_states)\n",
    "\n",
    "    # Select Q-values for the chosen actions\n",
    "    q_value = q_values.gather(1, torch.tensor(action_indices).unsqueeze(1))\n",
    "    \n",
    "    # Compute target Q-values\n",
    "    max_next_q_values = next_q_values.max(dim=1)[0]\n",
    "    target = rewards + (gamma * max_next_q_values) * (~dones)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = nn.MSELoss()(q_value.squeeze(), target)\n",
    "\n",
    "    # Optimize the Q-network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Main learning loop\n",
    "for episode in range(episodes):\n",
    "    basket_x = random.randint(0, WIDTH - basket_width)\n",
    "    apple_x = random.randint(0, WIDTH - apple_width)\n",
    "    basket_y = HEIGHT - 50\n",
    "    lives = 5\n",
    "    score = 0\n",
    "    episode_rewards = 0\n",
    "    episode_actions = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        state = get_state(basket_x, apple_x)\n",
    "        action = choose_action(state)\n",
    "\n",
    "        # Store action for the episode\n",
    "        episode_actions.append(action)\n",
    "\n",
    "        # Take action (move basket)\n",
    "        if action == 'left' and basket_x > 0:\n",
    "            basket_x -= basket_speed\n",
    "        elif action == 'right' and basket_x < WIDTH - basket_width:\n",
    "            basket_x += basket_speed\n",
    "\n",
    "        # Check if the apple was caught\n",
    "        if apple_x >= basket_x and apple_x <= basket_x + basket_width:\n",
    "            reward = 1  # Catching an apple\n",
    "            score += 1\n",
    "        else:\n",
    "            reward = -1  # Missing the apple or losing a life\n",
    "            lives -= 1  # Decrease lives if apple is missed\n",
    "\n",
    "        next_state = get_state(basket_x, apple_x)\n",
    "        done = lives <= 0  # Game over condition\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Sample a batch of experiences from the replay buffer\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            update_q_network(batch)\n",
    "\n",
    "        # Update target network periodically\n",
    "        if episode % update_target_every == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Track cumulative reward for this episode\n",
    "        episode_rewards += reward\n",
    "\n",
    "        # Check game over condition\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Score: {score}, Lives: {lives}, Rewards: {episode_rewards}\")\n",
    "    print(f\"Actions taken: {episode_actions}\")  # Print the actions taken in the episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108f957-b854-46d6-a207-29e80d49c65d",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e340973a-ef89-4ff0-8faa-9f3ef05d6b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Score: 38, Lives: 0\n",
      "Episode 2: Score: 0, Lives: 0\n",
      "Episode 3: Score: 0, Lives: 0\n",
      "Episode 4: Score: 18, Lives: 0\n",
      "Episode 5: Score: 0, Lives: 0\n",
      "Episode 6: Score: 0, Lives: 0\n",
      "Episode 7: Score: 0, Lives: 0\n",
      "Episode 8: Score: 0, Lives: 0\n",
      "Episode 9: Score: 0, Lives: 0\n",
      "Episode 10: Score: 0, Lives: 0\n",
      "Episode 11: Score: 0, Lives: 0\n",
      "Episode 12: Score: 0, Lives: 0\n",
      "Episode 13: Score: 0, Lives: 0\n",
      "Episode 14: Score: 0, Lives: 0\n",
      "Episode 15: Score: 0, Lives: 0\n",
      "Episode 16: Score: 0, Lives: 0\n",
      "Episode 17: Score: 0, Lives: 0\n",
      "Episode 18: Score: 0, Lives: 0\n",
      "Episode 19: Score: 0, Lives: 0\n",
      "Episode 20: Score: 0, Lives: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 20\n",
    "max_steps = 100\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99  # Discount factor\n",
    "clip_epsilon = 0.2\n",
    "update_epochs = 10\n",
    "batch_size = 64\n",
    "entropy_beta = 0.01\n",
    "\n",
    "# Screen dimensions\n",
    "WIDTH = 800\n",
    "HEIGHT = 600\n",
    "basket_width = 100\n",
    "basket_speed = 10\n",
    "apple_width = 20\n",
    "\n",
    "# Action space (left, right, stay)\n",
    "actions = ['left', 'right', 'stay']\n",
    "num_actions = len(actions)\n",
    "\n",
    "# Define the policy network (actor) and value network (critic)\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_size, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc_pi = nn.Linear(128, num_actions)  # Policy output\n",
    "        self.fc_v = nn.Linear(128, 1)  # Value output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        logits = self.fc_pi(x)\n",
    "        value = self.fc_v(x)\n",
    "        return logits, value\n",
    "\n",
    "# Initialize policy network\n",
    "actor_critic = ActorCritic(2, num_actions)\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "# Replay buffer for PPO\n",
    "class PPOBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def add(self, state, action, log_prob, reward, done, value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "# Helper functions\n",
    "def get_state(basket_x, apple_x):\n",
    "    return np.array([basket_x / WIDTH, apple_x / WIDTH], dtype=np.float32)\n",
    "\n",
    "def compute_advantages(rewards, values, dones, gamma):\n",
    "    advantages = []\n",
    "    returns = []\n",
    "    adv = 0\n",
    "    next_value = 0\n",
    "\n",
    "    for r, v, done in zip(reversed(rewards), reversed(values), reversed(dones)):\n",
    "        if done:\n",
    "            next_value = 0\n",
    "        delta = r + gamma * next_value - v\n",
    "        adv = delta + gamma * adv\n",
    "        returns.insert(0, r + gamma * next_value)\n",
    "        advantages.insert(0, adv)\n",
    "        next_value = v\n",
    "\n",
    "    return advantages, returns\n",
    "\n",
    "def ppo_update(buffer):\n",
    "    states = torch.tensor(np.array(buffer.states), dtype=torch.float32)\n",
    "    actions = torch.tensor(buffer.actions, dtype=torch.long)\n",
    "    log_probs = torch.tensor(buffer.log_probs, dtype=torch.float32)\n",
    "    rewards = torch.tensor(buffer.rewards, dtype=torch.float32)\n",
    "    dones = torch.tensor(buffer.dones, dtype=torch.bool)\n",
    "    values = torch.tensor(buffer.values, dtype=torch.float32).detach()\n",
    "\n",
    "    advantages, returns = compute_advantages(buffer.rewards, buffer.values, buffer.dones, gamma)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    for _ in range(update_epochs):\n",
    "        logits, new_values = actor_critic(states)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        ratios = torch.exp(new_log_probs - log_probs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = nn.MSELoss()(new_values.squeeze(), returns)\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - entropy_beta * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "buffer = PPOBuffer()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    basket_x = random.randint(0, WIDTH - basket_width)\n",
    "    apple_x = random.randint(0, WIDTH - apple_width)\n",
    "    basket_y = HEIGHT - 50\n",
    "    lives = 5\n",
    "    score = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        state = get_state(basket_x, apple_x)\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        logits, value = actor_critic(state_tensor)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample().item()\n",
    "        log_prob = dist.log_prob(torch.tensor(action)).item()\n",
    "\n",
    "        # Take action\n",
    "        if action == 0 and basket_x > 0:\n",
    "            basket_x -= basket_speed\n",
    "        elif action == 1 and basket_x < WIDTH - basket_width:\n",
    "            basket_x += basket_speed\n",
    "\n",
    "        # Check if apple is caught\n",
    "        reward = 1 if apple_x >= basket_x and apple_x <= basket_x + basket_width else -1\n",
    "        score += reward if reward > 0 else 0\n",
    "        lives -= 1 if reward == -1 else 0\n",
    "\n",
    "        done = lives <= 0\n",
    "\n",
    "        next_state = get_state(basket_x, apple_x)\n",
    "\n",
    "        # Add to buffer\n",
    "        buffer.add(state, action, log_prob, reward, done, value.item())\n",
    "\n",
    "        if done or step == max_steps - 1:\n",
    "            ppo_update(buffer)\n",
    "            buffer.clear()\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Score: {score}, Lives: {lives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11deb1aa-5b02-4d44-ab44-ba88a0601e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'stay', 'stay', 'right', 'right']\n",
      "Episode 2: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'stay', 'stay', 'stay', 'right']\n",
      "Episode 3: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'right', 'stay', 'stay', 'stay']\n",
      "Episode 4: Score: 0, Rewards: -5\n",
      "Actions taken: ['right', 'right', 'stay', 'right', 'right']\n",
      "Episode 5: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'left', 'stay', 'stay', 'stay']\n",
      "Episode 6: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'right', 'left', 'right', 'right']\n",
      "Episode 7: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'left', 'left', 'left', 'right']\n",
      "Episode 8: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'right', 'left', 'right', 'left']\n",
      "Episode 9: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'left', 'right', 'stay', 'stay']\n",
      "Episode 10: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'right', 'left', 'right', 'right']\n",
      "Episode 11: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'left', 'left', 'left', 'stay']\n",
      "Episode 12: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'stay', 'right', 'right', 'left']\n",
      "Episode 13: Score: 0, Rewards: -5\n",
      "Actions taken: ['right', 'stay', 'right', 'stay', 'right']\n",
      "Episode 14: Score: 0, Rewards: -5\n",
      "Actions taken: ['stay', 'left', 'left', 'left', 'left']\n",
      "Episode 15: Score: 0, Rewards: -5\n",
      "Actions taken: ['left', 'right', 'right', 'stay', 'left']\n",
      "Episode 16: Score: 24, Rewards: 19\n",
      "Actions taken: ['stay', 'right', 'stay', 'right', 'right', 'right', 'right', 'stay', 'left', 'stay', 'right', 'right', 'right', 'right', 'left', 'right', 'right', 'left', 'left', 'left', 'right', 'stay', 'right', 'right', 'right', 'stay', 'right', 'right', 'right']\n",
      "Episode 17: Score: 100, Rewards: 100\n",
      "Actions taken: ['right', 'left', 'left', 'left', 'right', 'right', 'left', 'right', 'right', 'right', 'right', 'right', 'left', 'stay', 'right', 'left', 'left', 'right', 'left', 'right', 'left', 'right', 'right', 'stay', 'right', 'left', 'stay', 'right', 'left', 'right', 'stay', 'left', 'right', 'stay', 'right', 'right', 'left', 'left', 'left', 'stay', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'stay', 'right', 'stay', 'left', 'right', 'right', 'right', 'right', 'right', 'left', 'right', 'stay', 'right', 'left', 'left', 'right', 'right', 'right', 'right', 'right', 'left', 'right', 'right', 'right', 'stay', 'right', 'right', 'right', 'right', 'left', 'right', 'left', 'left', 'right', 'right', 'right', 'left', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'right', 'left', 'right', 'right']\n",
      "Episode 18: Score: 0, Rewards: -5\n",
      "Actions taken: ['right', 'left', 'right', 'left', 'right']\n",
      "Episode 19: Score: 56, Rewards: 51\n",
      "Actions taken: ['left', 'left', 'left', 'left', 'right', 'right', 'left', 'left', 'right', 'stay', 'right', 'right', 'right', 'left', 'stay', 'stay', 'stay', 'right', 'left', 'stay', 'left', 'right', 'right', 'right', 'left', 'stay', 'stay', 'right', 'right', 'right', 'stay', 'stay', 'stay', 'right', 'right', 'stay', 'left', 'left', 'left', 'right', 'left', 'right', 'stay', 'left', 'left', 'left', 'stay', 'right', 'right', 'right', 'right', 'left', 'stay', 'right', 'stay', 'stay', 'right', 'right', 'right', 'right', 'right']\n",
      "Episode 20: Score: 0, Rewards: -5\n",
      "Actions taken: ['right', 'stay', 'left', 'left', 'stay']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 20\n",
    "max_steps = 100\n",
    "clip_epsilon = 0.2  # PPO clipping parameter\n",
    "batch_size = 64\n",
    "update_epochs = 4  # Number of PPO training epochs\n",
    "\n",
    "# Screen dimensions\n",
    "WIDTH = 800\n",
    "HEIGHT = 600\n",
    "basket_width = 100\n",
    "basket_speed = 10\n",
    "apple_width = 20\n",
    "\n",
    "# Action space\n",
    "actions = ['left', 'right', 'stay']\n",
    "action_map = {action: idx for idx, action in enumerate(actions)}\n",
    "\n",
    "# Neural Network Model for PPO\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = nn.Linear(128, action_size)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# Initialize actor-critic model and optimizer\n",
    "actor_critic = ActorCritic(2, len(actions))  # Input: state (basket_x, apple_x)\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_state(basket_x, apple_x):\n",
    "    \"\"\"Get the current state as basket's and apple's positions.\"\"\"\n",
    "    return np.array([basket_x / WIDTH, apple_x / WIDTH], dtype=np.float32)\n",
    "\n",
    "def choose_action(state, actor_critic):\n",
    "    \"\"\"Choose action using the policy from the actor.\"\"\"\n",
    "    state = torch.tensor(state).unsqueeze(0)\n",
    "    logits, _ = actor_critic(state)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    return action, torch.log(probs[0, action])\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma, lambda_gae):\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE).\"\"\"\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lambda_gae * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return advantages\n",
    "\n",
    "def ppo_update(actor_critic, optimizer, states, actions, log_probs, returns, advantages, clip_epsilon, batch_size):\n",
    "    \"\"\"Update the actor-critic model using PPO.\"\"\"\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    log_probs = np.array(log_probs)\n",
    "    returns = np.array(returns)\n",
    "    advantages = np.array(advantages)\n",
    "\n",
    "    for _ in range(update_epochs):\n",
    "        indices = np.arange(len(states))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(0, len(states), batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            batch_states = torch.tensor(states[batch_indices], dtype=torch.float32)\n",
    "            batch_actions = torch.tensor(actions[batch_indices], dtype=torch.long)\n",
    "            batch_log_probs = torch.tensor(log_probs[batch_indices], dtype=torch.float32)\n",
    "            batch_returns = torch.tensor(returns[batch_indices], dtype=torch.float32)\n",
    "            batch_advantages = torch.tensor(advantages[batch_indices], dtype=torch.float32)\n",
    "\n",
    "            logits, values = actor_critic(batch_states)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            new_log_probs = torch.log(probs.gather(1, batch_actions.unsqueeze(1)).squeeze(1))\n",
    "\n",
    "            # Calculate the ratio\n",
    "            ratio = torch.exp(new_log_probs - batch_log_probs)\n",
    "\n",
    "            # Surrogate objective\n",
    "            surrogate1 = ratio * batch_advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * batch_advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), batch_returns)\n",
    "\n",
    "            # Combined loss\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            # Optimize the actor-critic model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(episodes):\n",
    "    basket_x = random.randint(0, WIDTH - basket_width)\n",
    "    apple_x = random.randint(0, WIDTH - apple_width)\n",
    "    basket_y = HEIGHT - 50\n",
    "    lives = 5\n",
    "    score = 0\n",
    "\n",
    "    states = []\n",
    "    actions_taken = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    dones = []\n",
    "    episode_rewards = 0\n",
    "    episode_actions = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        state = get_state(basket_x, apple_x)\n",
    "        action, log_prob = choose_action(state, actor_critic)\n",
    "        _, value = actor_critic(torch.tensor(state).unsqueeze(0))\n",
    "        values.append(value.item())\n",
    "\n",
    "        # Take action\n",
    "        if action == action_map['left'] and basket_x > 0:\n",
    "            basket_x -= basket_speed\n",
    "        elif action == action_map['right'] and basket_x < WIDTH - basket_width:\n",
    "            basket_x += basket_speed\n",
    "\n",
    "        # Check if the apple was caught\n",
    "        if apple_x >= basket_x and apple_x <= basket_x + basket_width:\n",
    "            reward = 1\n",
    "            score += 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            lives -= 1\n",
    "\n",
    "        next_state = get_state(basket_x, apple_x)\n",
    "        done = lives <= 0\n",
    "        episode_rewards += reward\n",
    "        episode_actions.append(actions[action])\n",
    "\n",
    "        # Store the transition\n",
    "        states.append(state)\n",
    "        actions_taken.append(action)\n",
    "        log_probs.append(log_prob.item())\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Add the final value for GAE computation\n",
    "    _, final_value = actor_critic(torch.tensor(next_state).unsqueeze(0))\n",
    "    values.append(final_value.item())\n",
    "\n",
    "    # Compute GAE and returns\n",
    "    advantages = compute_gae(rewards, values, dones, gamma, lambda_gae=0.95)\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "\n",
    "    # Update the model using PPO\n",
    "    ppo_update(actor_critic, optimizer, states, actions_taken, log_probs, returns, advantages, clip_epsilon, batch_size)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Score: {score}, Rewards: {episode_rewards}\")\n",
    "    print(f\"Actions taken: {episode_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9be5c-3eea-4c0c-95d4-bbcc3b71c656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
